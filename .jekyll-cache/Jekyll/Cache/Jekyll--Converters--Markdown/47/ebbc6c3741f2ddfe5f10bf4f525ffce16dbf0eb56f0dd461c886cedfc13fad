I"®<p>Here are some tools that I have been working on. They are all open source.</p>

<h1 id="prophecise">Prophecise</h1>
<p>A web app (prophecise.com) that leverages Facebook Prophet to create fast and robust time-series forecasts from your Google Analytics data using a simple UI.</p>

<p><strong>What is Prophecise?</strong></p>

<p><a href="https://prophecise.com">Prophecise</a> is an open source Flask web app that enables analysts and data scientists to quickly build accurate forecast models from their Google Analytics data. It builds upon the great work Gareth Cull has been doing with <a href="https://github.com/garethcull/forecastr">Forecastr</a>. It lets users sign in with their Google Analytics account and pull reports directly into the UI for forecasting. Here‚Äôs an overview of the data flow:</p>

<amp-img src="/assets/images/prophecise-data-flow.jpg" height="300" width="660" layout="responsive" alt="prophecise.com"></amp-img>
<p><br /></p>

<h1 id="ml-content-classification-for-ga">ML Content Classification for GA</h1>

<p>This app scrapes the content from websites using headless-chrome-crawler, runs the content through Google‚Äôs pre-trained content classification model and then uploads the results to Google Analytics using the Management API.</p>

<p><strong>How to set it up</strong></p>

<p>1) Create three custom dimensions in GA for <code class="language-plaintext highlighter-rouge">Primary Category</code>, <code class="language-plaintext highlighter-rouge">Subcategory</code> and <code class="language-plaintext highlighter-rouge">Secondary Subcategory</code></p>

<p>2) Set up a Data Import in GA with the type ‚ÄúContent‚Äù. For the data set schema: ‚ÄúKey‚Äù should be set to <code class="language-plaintext highlighter-rouge">Page</code> and ‚ÄúImported Data‚Äù should be set to the three new custom dimensions</p>

<p>3) Clone this repo <code class="language-plaintext highlighter-rouge">git clone https://github.com/alsjohnstone/scraper-classification.git</code></p>

<p>4) Create a GCP project, enable NLP API and Analytics API</p>

<p>5) Create service worker account and download the credentials.json file</p>

<p>6) Update the config.json file</p>

<p>7) Create a new GCP storage bucket and add your config.json and credentials.json files</p>

<p>8) Update the bucket referenced in the install.sh file with your bucket</p>

<p>9) Run the command below to spin up a new GCP Compute Instance that will automatically shutdown after the crawl is complete.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gcloud compute instances create scraper \
    --machine-type=n1-standard-16 \
    --metadata-from-file=startup-script=./install.sh \
    --scopes=cloud-platform \
    --zone=europe-west2-c
</code></pre></div></div>

<p>Restarting the GCP Compute Instance will automatically scrape, classify and upload the results to GA. You could also schedule crawls using Cloud Scheduler.</p>
:ET